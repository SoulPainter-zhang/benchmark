# å¤ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®ºæ–‡ï¼ˆæŠ€æœ¯æŠ¥å‘Šï¼‰ä¸­çš„æ•°æ®é›†æµ‹è¯„ç»“æœï¼ˆä»¥DeepSeek R1ä½¿ç”¨çš„GPQAæ•°æ®é›†ä¸ºä¾‹ï¼‰
## å‰è¨€-æ–¹æ³•è®º
å¦‚æœæƒ³è¦é€šè¿‡AISBenchæµ‹è¯„å·¥å…·å¤ç°è®ºæ–‡ç²¾åº¦ï¼Œéœ€è¦å¯¹é½æ¨¡å‹çš„æŠ€æœ¯æŠ¥å‘Šæˆ–è®ºæ–‡ä¸­å¯¹æ­¤æ•°æ®é›†çš„æµ‹è¯•æ–¹æ³•ï¼Œåœ¨è¯„æµ‹å·¥å…·è¿™è¾¹éœ€è¦å¯¹é½çš„å¦‚ä¸‹ï¼š
**æ¨¡å‹ç›¸å…³é…ç½®**ï¼š
- é€‰å–åˆé€‚çš„endpointå¯¹åº”çš„æ¨¡å‹ä»»åŠ¡
- æœ€å¤§è¾“å‡ºé•¿åº¦å®Œå…¨å¯¹é½
- åå¤„ç†å‚æ•°å®Œå…¨å¯¹é½

**æ•°æ®é›†ç›¸å…³é…ç½®**
- æç¤ºè¯å·¥ç¨‹å®Œå…¨å¯¹é½
- ç­”æ¡ˆæå–æ–¹å¼å®Œå…¨å¯¹é½
- ç²¾åº¦è¯„ä¼°æŒ‡æ ‡å¯¹é½

## ä»¥å¤ç°DeepSeek R1æ¨¡å‹åœ¨GPQAæ•°æ®é›†ä¸Šçš„æµ‹è¯„ç»“æœä¸ºä¾‹
### é€‰å–åˆé€‚çš„endpointå¯¹åº”çš„æ¨¡å‹é…ç½®æ–‡ä»¶
ä¸ºäº†ç¡®ä¿æ‰§è¡Œæ•ˆç‡ï¼Œå¤ç°æ¨¡å‹ç²¾åº¦æ—¶ä¸€èˆ¬ä¹Ÿä½¿ç”¨æ¨ç†æœåŠ¡ä½œä¸ºè¢«æµ‹å¯¹è±¡ã€‚è®¿é—®æ¨ç†æœåŠ¡å¯ä»¥æœ‰ä¸åŒçš„endpointï¼Œä¸šç•Œä¸»æµè¿˜æ˜¯ç”¨OpenAIçš„endpointã€‚OpenAIçš„endpointä¸»è¦æœ‰ä¸¤ç§ï¼š`v1/completions`å’Œ`v1/chat/completions`ã€‚
- **v1/completions**ï¼š
    æ¨¡å‹æŒ‰ â€œç»­å†™å‰ç¼€â€ é€»è¾‘ç”Ÿæˆæ–‡æœ¬ï¼Œä¸ä¸»åŠ¨åŒºåˆ† â€œæŒ‡ä»¤â€ ä¸ â€œå†…å®¹â€ï¼Œéœ€é€šè¿‡ prompt å·¥ç¨‹å¼ºå¼•å¯¼ï¼ˆå¦‚åŠ  â€œè¯·å›ç­”ï¼šâ€ï¼‰ï¼Œå¦åˆ™å¯èƒ½å‡ºç°æ¨¡ä»¿å¼è¾“å‡ºè€ŒéæŒ‡ä»¤æ‰§è¡Œã€‚ä¾‹å¦‚è¾“å…¥ â€œå°†ä¸‹åˆ—è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼šHelloâ€ï¼Œå¯èƒ½ç»­å†™ä¸º â€œå°†ä¸‹åˆ—ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½â€ï¼Œè€Œéç›´æ¥ç¿»è¯‘ã€‚
    å› æ­¤é€‚åˆå•è½®æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ä»£ç è¡¥å…¨ã€çŸ­æ–‡å†™ä½œã€æ–‡æœ¬ç»­å†™ã€ç®€å•æ–‡æœ¬åˆ†ç±»ï¼‰ï¼Œæˆ–éœ€å…¼å®¹æ—§ç‰ˆåŸºç¡€æ¨¡å‹çš„åœºæ™¯ã€‚
- **v1/chat/completions**ï¼š
    æ¨¡å‹åŸç”Ÿç†è§£ system/user/assistant è§’è‰²è¯­ä¹‰ï¼Œä¼˜å…ˆæ‰§è¡Œç”¨æˆ·æŒ‡ä»¤ï¼Œå¯¹è¯ä¸€è‡´æ€§ä¸æ„å›¾å¯¹é½æ›´ç¨³å®šï¼Œæ— éœ€å¤æ‚ prompt åŒ…è£…å³å¯å®Œæˆç¿»è¯‘ã€æ€»ç»“ç­‰ä»»åŠ¡ã€‚
    å› æ­¤é€‚åˆå¤šè½®å¯¹è¯ï¼ˆå®¢æœã€èŠå¤©æœºå™¨äººï¼‰ã€æŒ‡ä»¤é©±åŠ¨ä»»åŠ¡ï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€æ•°æ®åˆ†æï¼‰ã€å·¥å…·é›†æˆï¼ˆå‡½æ•°è°ƒç”¨ã€æ£€ç´¢å¢å¼ºï¼‰ã€å¤šæ¨¡æ€äº¤äº’ç­‰ç°ä»£ LLM åº”ç”¨åœºæ™¯ã€‚

ğŸ’¡ ç›®å‰ï¼ˆ2025å¹´1æœˆä¹‹åï¼‰ï¼Œå‡ ä¹æ‰€æœ‰æ–°å‘å¸ƒçš„LLMæ¨¡å‹éƒ½æ”¯æŒ`v1/chat/completions` endpointï¼Œè€Œä¸”`v1/completions` endpointä¹ŸåŸºæœ¬è¢«åºŸå¼ƒï¼Œå› æ­¤æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­ä¸€èˆ¬åªé€‰ç”¨è®¿é—®`v1/chat/completions` endpointçš„æ¨¡å‹ä»»åŠ¡**vllm_api_general_chat**(ç”¨éæµå¼æ¥å£è®¿é—®æœåŠ¡)å’Œ**vllm_api_stream_chat**(ç”¨æµå¼æ¥å£è®¿é—®æœåŠ¡)ã€‚

ä»¥æ¨¡å‹ä»»åŠ¡`vllm_api_general_chat`ä¸ºä¾‹ï¼Œå…¶å¯¹åº”çš„æ¨¡å‹é…ç½®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„å¯ä»¥é€šè¿‡æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å¾—åˆ°ï¼š
```bash
ais_bench --models vllm_api_general_chat --search
```

âš ï¸ åç»­æ¨¡å‹ç›¸å…³é…ç½®éƒ½åœ¨æ­¤é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹ã€‚


### æœ€å¤§è¾“å‡ºé•¿åº¦å®Œå…¨å¯¹é½
åœ¨[DeepSeek R1 Huggingfaceæ¨¡å‹å¡ç‰‡](https://huggingface.co/deepseek-ai/DeepSeek-R1)ä¸­å¯ä»¥çœ‹åˆ°å¦‚ä¸‹æè¿°
> ## 4. Evaluation Results
> ### DeepSeek-R1-Evaluation
> For all our models, the maximum generation length is set to 32,768 tokens....

ä»è¿™æ®µæè¿°å¯ä»¥çœ‹å‡ºï¼ŒDeepSeek R1æ¨¡å‹çš„æœ€å¤§è¾“å‡ºé•¿åº¦è¢«è®¾ç½®ä¸º32,768 tokensã€‚

å› æ­¤ä»¥æ¨¡å‹ä»»åŠ¡`vllm_api_general_chat`ä¸ºä¾‹ï¼Œæœ€å¤§è¾“å‡ºé•¿åº¦çš„é…ç½®å¦‚ä¸‹ï¼š
```python
from ais_bench.benchmark.models import VLLMCustomAPIChat

models = [
    dict(
        attr="service",
        type=VLLMCustomAPIChat,
        abbr='vllm-api-general-chat',
        # ......
        max_out_len=32768,          # æ¨ç†æœåŠ¡è¾“å‡ºçš„tokençš„æœ€å¤§æ•°é‡
        # ......
    )
]
```

### åå¤„ç†å‚æ•°å®Œå…¨å¯¹é½
åœ¨[DeepSeek R1 Huggingfaceæ¨¡å‹å¡ç‰‡](https://huggingface.co/deepseek-ai/DeepSeek-R1)ä¸­å¯ä»¥çœ‹åˆ°å¦‚ä¸‹æè¿°
> ## 4. Evaluation Results
> ### DeepSeek-R1-Evaluation
> ..., For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, ...

ä»è¿™æ®µæè¿°å¯ä»¥çœ‹å‡ºï¼ŒDeepSeek R1æ¨¡å‹çš„åå¤„ç†å‚æ•°åŒ…æ‹¬æ¸©åº¦ï¼ˆtemperatureï¼‰ä¸º0.6ï¼Œtop-på€¼ï¼ˆtop_pï¼‰ä¸º0.95ã€‚

å› æ­¤ä»¥æ¨¡å‹ä»»åŠ¡`vllm_api_general_chat`ä¸ºä¾‹ï¼Œåå¤„ç†å‚æ•°çš„é…ç½®å¦‚ä¸‹ï¼š
```python
from ais_bench.benchmark.models import VLLMCustomAPIChat

models = [
    dict(
        attr="service",
        type=VLLMCustomAPIChat,
        abbr='vllm-api-general-chat',
        # ......
        generation_kwargs=dict( # åå¤„ç†å‚æ•°éƒ½å¡«åœ¨è¿™é‡Œ
            temperature=0.6,
            top_p=0.95,
        ),
        # ......
    )
]
```

### æç¤ºè¯å·¥ç¨‹å®Œå…¨å¯¹é½
æç¤ºè¯å·¥ç¨‹å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“æ˜¯å¾ˆæ˜¾è‘—çš„ï¼Œä¸€èˆ¬æ¨¡å‹è®ºæ–‡æˆ–æŠ€æœ¯æŠ¥å‘Šéƒ½ä¼šå…¬å¼€æµ‹è¯•æ‰€ä½¿ç”¨çš„æç¤ºè¯ï¼Œå¦‚æœæ˜¯ä½¿ç”¨ç¬¬ä¸‰æ–¹å·¥å…·æµ‹è¯•çš„ï¼Œä¹Ÿä¼šå¼ºè°ƒå…·ä½“æ˜¯ä»€ä¹ˆå¼€æºå·¥å…·ã€‚
åœ¨DeepSeek R1è®ºæ–‡ä¸­ï¼ŒæåŠäº†GPQAæ•°æ®é›†çš„æµ‹è¯•æç¤ºè¯å·¥ç¨‹ï¼Œå¦‚ä¸‹ï¼š

> Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQADiamond, and SimpleQA are evaluated using prompts from the simple-evals framework.

è¿™è¯´æ˜DeepSeek R1çš„æç¤ºè¯å·¥ç¨‹æ˜¯ä½¿ç”¨simple-evalsè¿™ä¸ªå·¥å…·çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å‚è€ƒ[simple-evals](https://github.com/openai/simple-evals)è¿™ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­GPQAç”¨åˆ°çš„æç¤ºè¯ç›¸å…³çš„éƒ¨åˆ†ä¸º(éœ€æ±‚å»ä»£ç é‡Œçœ‹)ï¼š
```python
QUERY_TEMPLATE_MULTICHOICE = """
Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.

{Question}

A) {A}
B) {B}
C) {C}
D) {D}
""".strip()
```

å› æ­¤AISBenchçš„æç¤ºè¯å·¥ç¨‹åº”å½“ä¿®æ”¹ä¸º:
```python
# https://github.com/AISBench/benchmark/blob/master/ais_bench/benchmark/configs/datasets/gpqa/gpqa_gen_0_shot_cot_chat_prompt.py

## ä¸simple-evalså®Œå…¨ç›¸åŒçš„æç¤ºè¯æ¨¡æ¿
align_prompt = """
Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.

{question}

A) {A}
B) {B}
C) {C}
D) {D}
""".strip()

## ......

gpqa_infer_cfg = dict(
    prompt_template=dict( # æç¤ºè¯å·¥ç¨‹
        type=PromptTemplate,
        template=dict(
            round=[
                dict(role='HUMAN', prompt=align_prompt), # ä¼ å…¥æç¤ºè¯æ¨¡æ¿
            ], )),
    retriever=dict(type=ZeroRetriever),
    inferencer=dict(type=GenInferencer))
```
æ›´è¯¦ç»†çš„æç¤ºè¯å·¥ç¨‹çš„ä»‹ç»è¯·å‚è€ƒ[Prompt æ¨¡æ¿ä»‹ç»](../prompt/prompt_template.md)

### ç­”æ¡ˆæå–æ–¹å¼å®Œå…¨å¯¹é½
å¦‚ä½•ä»æ¨¡å‹çš„æ¨ç†ç»“æœä¸­æå–ç­”æ¡ˆå¹¶è¯„ä¼°ä¼šç›´æ¥å½±å“è¯„æµ‹çš„å¾—åˆ†ã€‚llmæµ‹è¯„ä½¿ç”¨çš„æ•°æ®é›†çš„ç­”æ¡ˆæå–æ€»ä½“æ¥è¯´åˆ†ä¸º3ç±»ï¼š
1. å¯¹äºç±»ä¼¼é€‰æ‹©é¢˜æˆ–é—®ç­”é¢˜ç±»å‹çš„è¯„æµ‹æ•°æ®é›†ï¼ˆcevalã€gsm8kç­‰ï¼‰ï¼Œç­”æ¡ˆæå–æ–¹å¼ä¸€èˆ¬æ˜¯åŸºäºå›ºå®šçš„æ­£åˆ™è¡¨è¾¾å¼ã€‚
2. å¯¹äºæ¯”è¾ƒå¤æ‚çš„è¯¸å¦‚ä»£ç ç±»æˆ–è€…è¦åŒ…å«è§£é¢˜è¿‡ç¨‹çš„æ•°å­¦ç±»çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚livecodebenchã€humanevalã€math500ç­‰ï¼‰ï¼Œä¸€èˆ¬ä¼šæœ‰ç»Ÿä¸€çš„æ•°æ®é›†é…å¥—çš„evaluateåº“ä¾›è°ƒç”¨ã€‚
3. å¯¹äºä¸€äº›å‘æ•£æˆ–è€…åä¸»è§‚çš„æ•°æ®é›†å¯èƒ½éœ€è¦å¼•å…¥è£åˆ¤æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

ä¸€èˆ¬æƒ…å†µä¸‹åªæœ‰æµ‹è¯„æ¶‰åŠåˆ°ç¬¬3ç±»æ•°æ®é›†æ—¶ï¼Œåœ¨æ¨¡å‹çš„è®ºæ–‡æˆ–æŠ€æœ¯æŠ¥å‘Šé‡Œæ‰ä¼šæ˜ç¡®è¯´æ˜ï¼ˆä¾‹å¦‚ä½¿ç”¨GPT-4-1106çš„apiä½œä¸ºè£åˆ¤æ¨¡å‹ï¼‰ã€‚å¯¹äºç¬¬2ç±»æ•°æ®é›†ä¸€èˆ¬é»˜è®¤ä½¿ç”¨æ•°æ®é›†é…å¥—çš„evaluateåº“è¿›è¡Œç­”æ¡ˆæå–ã€‚

å¯¹äºç¬¬1ç±»æ•°æ®é›†ï¼Œå¦‚æœæ¨¡å‹è®ºæ–‡æˆ–æŠ€æœ¯æŠ¥å‘Šè¯´æ˜äº†æç¤ºè¯å·¥ç¨‹å‡ºè‡ªä»€ä¹ˆå·¥å…·ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥æ²¿ç”¨è¿™ä¸ªå·¥å…·ä¸­æåŠçš„ç­”æ¡ˆæå–æ–¹å¼ï¼Œè¿˜æ˜¯ä»¥GPQAä¸ºä¾‹ï¼Œåœ¨simple-evalsä¸­ï¼ŒGPQAçš„ç­”æ¡ˆæå–æ–¹å¼å°±æ˜¯åŸºäºå›ºå®šçš„æ­£åˆ™è¡¨è¾¾å¼:
```python
# https://github.com/openai/simple-evals/blob/main/common.py
ANSWER_PATTERN_MULTICHOICE = r"(?i)Answer[ \t]*:[ \t]*\$?([A-D])\$?"

# https://github.com/openai/simple-evals/blob/main/gpqa_eval.py
match = re.search(ANSWER_PATTERN_MULTICHOICE, response_text)
```
å› æ­¤AISBenchçš„ç­”æ¡ˆæå–æ–¹å¼åº”å½“ä¿®æ”¹ä¸ºä¸simple-evalså®Œå…¨ç›¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼ï¼š
```python
# https://github.com/AISBench/benchmark/blob/master/ais_bench/benchmark/datasets/gpqa.py
@TEXT_POSTPROCESSORS.register_module() # ç­”æ¡ˆæå–å‡½æ•°ï¼Œä»æ¨¡å‹åŸå§‹å›ç­”çš„å­—ç¬¦ä¸²ä¸­æå–A,B,C,Dä¸­ä¸€ä¸ªé€‰é¡¹
def GPQA_Simple_Eval_postprocess(text: str) -> str:
    """
    ä»æ¨¡å‹åŸå§‹å›ç­”çš„å­—ç¬¦ä¸²ä¸­æå–A,B,C,Dä¸­ä¸€ä¸ªé€‰é¡¹ä½œä¸ºç­”æ¡ˆã€‚

    :param text: æ¨¡å‹åŸå§‹å›ç­”çš„å­—ç¬¦ä¸²ã€‚
    :return: æå–åˆ°çš„ç­”æ¡ˆé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰ï¼Œå¦‚æœæœªæ‰¾åˆ°åŒ¹é…é¡¹åˆ™è¿”å›Noneã€‚
    """
    ANSWER_PATTERN = r"(?i)Answer[ \t]*:[ \t]*\$?([A-D])\$?"
    match = re.search(ANSWER_PATTERN, text)
    if match:
        return match.group(1)
    return None


# https://github.com/AISBench/benchmark/blob/master/ais_bench/benchmark/configs/datasets/gpqa/gpqa_gen_0_shot_cot_chat_prompt.py

from ais_bench.benchmark.datasets import GPQADataset, GPQA_Simple_Eval_postprocess, GPQAEvaluator

gpqa_eval_cfg = dict(evaluator=dict(type=GPQAEvaluator),
                     pred_postprocessor=dict(type=GPQA_Simple_Eval_postprocess)) # ä¼ å…¥è‡ªå®šä¹‰çš„ç­”æ¡ˆæå–å‡½æ•°ï¼Œå‡½æ•°æœ¬èº«ä¹Ÿå¯ä»¥ç›´æ¥å®šä¹‰åœ¨æ•°æ®é›†é…ç½®æ–‡ä»¶é‡Œ
```

### ç²¾åº¦è¯„ä¼°æŒ‡æ ‡å¯¹é½
ä¸€èˆ¬æ¨¡å‹çš„è¯„æµ‹ç»“æœéƒ½ä¼šæœ‰è¿™æ ·ä¸€å¼ è¡¨ï¼Œä»¥DeepSeekçš„ä¸ºä¾‹ï¼š

|Model|	AIME 2024 pass@1|	AIME 2024 cons@64|	MATH-500 pass@1|	GPQA Diamond pass@1|	LiveCodeBench pass@1|CodeForces rating|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
|GPT-4o-0513|	9.3|	13.4|	74.6|	49.9|	32.9|	759|
|Claude-3.5-Sonnet-1022|	16.0|	26.7|	78.3|	65.0|	38.9|	717|
|o1-mini|	63.6|	80.0|	90.0|	60.0|	53.8|	1820|

å…¶ä¸­çš„`cons@64`ï¼Œ`pass@1`å°±æ˜¯è¡¨ç¤ºç²¾åº¦è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›ç²¾åº¦è¯„ä¼°æŒ‡æ ‡çš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è€ƒ[ç²¾åº¦è¯„ä¼°æŒ‡æ ‡è¯´æ˜](../base_tutorials/results_intro/accuracy_metric.md#äºŒpassk-consk-avgn-çš„å®šä¹‰ä¸å…³ç³»)

ä»¥GPQAä¸ºä¾‹ï¼Œè¡¨æ ¼æ˜¯æŒ‡æ˜å…¶æ˜¯ä½¿ç”¨`pass@1`ä½œä¸ºç²¾åº¦è¯„ä¼°æŒ‡æ ‡çš„ï¼Œåœ¨DeepSeek R1è®ºæ–‡ä¸­å¯¹pass@1çš„æè¿°å¦‚ä¸‹ï¼š

> ... , and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-ğ‘ value of 0.95 to generate ğ‘˜ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as
>  ${\text{pass@1}} = \frac{1}{n} \sum_{i=1}^{n} p_i$

é‚£ä¹ˆåœ¨AISBenchä¸­ï¼Œæ¨¡å‹é…ç½®æ–‡ä»¶åšå¦‚ä¸‹é…ç½®ï¼š
```python
# https://github.com/AISBench/benchmark/blob/master/ais_bench/benchmark/configs/models/vllm_api/vllm_api_stream_chat.py

models = [
    dict(
        ... # å…¶å®ƒå‚æ•°
        generation_kwargs = dict(
            num_return_sequences = 4, # n=4~64
            ... # å…¶å®ƒå‚æ•°
        ),
        ...
    )
]

```
ä¸€èˆ¬æƒ…å†µä¸‹ `n == k` æˆ–è€… `k=1`ï¼Œ`n == k`çš„åœºæ™¯æ¨ç†å‡ºçš„æŒ‡æ ‡ä¸º`path@k`ï¼Œ`k=1`çš„åœºæ™¯ä¹Ÿå°±æ˜¯deepseekå…¬å¼ä¸­çš„`pass@1`æœ¬è´¨ä¸Šæ˜¯`avg@n`ï¼Œå•ç‹¬é…ç½®`n`å¤Ÿç”¨ï¼Œå› æ­¤AISBenchè¯„æµ‹å·¥å…·20251219ç‰ˆæœ¬ä¸‹è¿˜ä¸æ”¯æŒå•ç‹¬é…ç½®`k`ã€‚

ç²¾åº¦è¯„ä¼°é˜¶æ®µç»“æŸåï¼Œç»“æœä¼šè®°å½•åœ¨æ—¥å¿—å’Œæ‰“å±åœ¨è¿è¡Œçª—å£ï¼Œæ ¼å¼æŒ‰ç…§ä»¥ä¸‹ç¤ºä¾‹å†…å®¹ï¼ˆæ•°æ®ä»…ä¾›å‚è€ƒï¼‰ï¼š

```bash
| dataset   | version   | metric                    | mode | vllm-api-stream-chat |
| --------- | --------- | ------------------------- | ---- | -------------------- |
| GPQA_diamond | 604a78    | accuracy (4 runs average) | gen  | 18.00                |
| GPQA_diamond | 604a78    | avg@4                     | gen  | 18.00                |
| GPQA_diamond | 604a78    | pass@4                    | gen  | 53.33                |
| GPQA_diamond | 604a78    | cons@4                    | gen  | 13.33                |
```
å…¶ä¸­`avg@4`å’Œdeepseekä¸­æ‰§è¡Œ4æ¬¡å¹³å‡çš„`pass@1`å«ä¹‰ç›¸åŒã€‚

> âš ï¸ `n`æœ¬èº«åªæ˜¯å½±å“è¯„æµ‹ç»“æœçš„æ³¢åŠ¨å¹…åº¦ä¸å½±å“æ•°å­¦æœŸæœ›ï¼Œä½†æ˜¯`n`è¶Šå¤§æ„å‘³è¿™éœ€è¦åå¤è·‘åŒä¸€æ¡ç”¨ä¾‹çš„æ¬¡æ•°è¶Šå¤šï¼Œèµ„æºæ¶ˆè€—è¶Šå¤§ã€‚å¦‚æœè¦å¤ç°ç²¾åº¦éœ€è¦ä¾æ®å®é™…èµ„æºæƒ…å†µåšè°ƒæ•´ã€‚

> ğŸ’¡å¦‚æœè®ºæ–‡çš„æ•°æ®é›†ä¸å¼ºè°ƒæ˜¯ä»€ä¹ˆç²¾åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œä¸€èˆ¬é»˜è®¤`pass@1`ï¼Œå› æ­¤åœ¨AISBenchçš„æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½®`n`å’Œ`k`å°±æ˜¯é»˜è®¤`pass@1`

## å‚è€ƒèµ„æ–™
- DeepSeek R1 huggingfaceæ¨¡å‹å¡ç‰‡ï¼šhttps://huggingface.co/deepseek-ai/DeepSeek-R1
- DeepSeek R1 è®ºæ–‡ï¼šhttps://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf